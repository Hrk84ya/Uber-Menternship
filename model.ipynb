{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6dc8b5",
   "metadata": {},
   "source": [
    "# Traffic Prediction Pipeline\n",
    "This notebook implements a TrafficPredictor class for time series forecasting, feature engineering, model training, interpretation, and error analysis using XGBoost, Optuna, and SHAP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec6868d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "import shap\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7859c",
   "metadata": {},
   "source": [
    "## TrafficPredictor Class\n",
    "Encapsulates all modeling and analysis steps, including feature engineering, model training, interpretability, and error visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c55d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficPredictor:\n",
    "    def __init__(self, data_path, output_dir='results'):\n",
    "        self.data_path = data_path\n",
    "        self.output_dir = output_dir\n",
    "        self._create_output_dir()\n",
    "        self.df = None\n",
    "        self.X_train, self.X_test = None, None\n",
    "        self.y_train, self.y_test = None, None\n",
    "        self.feature_names = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.models = {}\n",
    "        self.metrics = {}\n",
    "        self.best_params = {}\n",
    "        self.chunk_size = 10000\n",
    "        self.study = None\n",
    "\n",
    "    def _create_output_dir(self):\n",
    "        for subdir in ['models', 'plots', 'shap']:\n",
    "            os.makedirs(os.path.join(self.output_dir, subdir), exist_ok=True)\n",
    "\n",
    "    def _add_time_features(self, df):\n",
    "        \"\"\"\n",
    "        Adds time-based features to dataframe.\n",
    "        Tries to capture temporal cycles like hour of day, month, etc.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        # Cyclical hour encoding\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 23.0)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 23.0)\n",
    "        # Datetime features\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        df['day_of_month'] = df.index.day\n",
    "        df['month'] = df.index.month\n",
    "        df['year'] = df.index.year\n",
    "        # Cyclical day/month\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df.index.dayofyear / 365.0)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df.index.dayofyear / 365.0)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df.index.month / 12.0)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df.index.month / 12.0)\n",
    "        # Relative time\n",
    "        df['days_since_start'] = (df.index - df.index.min()).days\n",
    "        # Lag features for vehicles\n",
    "        for lag in [1, 2, 3, 24, 168]:\n",
    "            df[f'vehicles_lag_{lag}'] = df['Vehicles'].shift(lag)\n",
    "        # Rolling averages\n",
    "        for window in [3, 6, 12, 24]:\n",
    "            df[f'vehicles_rolling_mean_{window}'] = df['Vehicles'].rolling(window).mean()\n",
    "            df[f'vehicles_rolling_std_{window}'] = df['Vehicles'].rolling(window).std()\n",
    "        # Drop rows with NA after lag/roll\n",
    "        df = df.dropna()\n",
    "        return df\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        chunks = []\n",
    "        # Read in manageable chunks for large files\n",
    "        for chunk in pd.read_csv(self.data_path, chunksize=self.chunk_size,\n",
    "                                usecols=['DateTime','Junction','Vehicles','Hour','Avg_Temp','Precipitation','Wind_Speed']):\n",
    "            chunk['DateTime'] = pd.to_datetime(chunk['DateTime'])\n",
    "            chunk.sort_values('DateTime', inplace=True)\n",
    "            chunks.append(chunk)\n",
    "        self.df = pd.concat(chunks)\n",
    "        self.df.set_index('DateTime', inplace=True)\n",
    "        self.df.sort_index(inplace=True)\n",
    "        # Add temporal features\n",
    "        self.df = self._add_time_features(self.df)\n",
    "        self.feature_names = [col for col in self.df.columns if col != 'Vehicles']\n",
    "        print(f\"Feature count: {len(self.feature_names)}\")\n",
    "        return self.df\n",
    "\n",
    "    def objective(self, trial, X, y):\n",
    "        \"\"\"Optuna objective for hyperparameter tuning.\"\"\"\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            model = XGBRegressor(**params)\n",
    "            model.fit(X[train_idx], y[train_idx])\n",
    "            pred = model.predict(X[val_idx])\n",
    "            cv_scores.append(np.sqrt(mean_squared_error(y[val_idx], pred)))\n",
    "        return np.mean(cv_scores)\n",
    "\n",
    "    def optimize_hyperparameters(self, n_trials=15):\n",
    "        print(\"\\nRunning Optuna for hyperparam tuning...\")\n",
    "        X = self.scaler.fit_transform(self.X_train)\n",
    "        y = self.y_train.values\n",
    "        self.study = optuna.create_study(direction='minimize')\n",
    "        self.study.optimize(lambda t: self.objective(t, X, y), n_trials=n_trials, n_jobs=-1)\n",
    "        self.best_params = self.study.best_params\n",
    "        print(\"Best params:\", self.best_params)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        hist = self.study.trials_dataframe()\n",
    "        plt.plot(hist['number'], hist['value'], marker='o')\n",
    "        plt.title('Optuna Optimization History')\n",
    "        plt.xlabel('Trial')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{self.output_dir}/plots/opt_history.png\")\n",
    "        plt.close()\n",
    "        return self.best_params\n",
    "\n",
    "    def train_xgboost(self, use_optimization=True):\n",
    "        print(\"Training XGBoost...\")\n",
    "        if use_optimization and not self.best_params:\n",
    "            self.optimize_hyperparameters()\n",
    "        params = self.best_params or {\n",
    "            'n_estimators': 150,\n",
    "            'learning_rate': 0.1,\n",
    "            'max_depth': 5,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        joblib.dump(model, f\"{self.output_dir}/models/xgb_model.pkl\")\n",
    "        metrics = self.evaluate_model(model, self.X_test, self.y_test, 'XGBRegressor')\n",
    "        self._plot_feature_importance(model)\n",
    "        self._shap_analysis(model)\n",
    "        self.models['xgboost'] = model\n",
    "        return model, metrics\n",
    "\n",
    "    def _plot_feature_importance(self, model):\n",
    "        impt = model.feature_importances_\n",
    "        order = np.argsort(impt)[::-1]\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.barh(range(20), impt[order][:20][::-1])\n",
    "        plt.yticks(range(20), [self.feature_names[i] for i in order][:20][::-1])\n",
    "        plt.title(\"Top 20 Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/plots/feature_importance.png\")\n",
    "        plt.close()\n",
    "        pd.DataFrame({'feature':[self.feature_names[i] for i in order],'importance':impt[order]})\\\n",
    "            .to_csv(f\"{self.output_dir}/top_features.csv\", index=False)\n",
    "\n",
    "    def _shap_analysis(self, model, sample_size=1000):\n",
    "        \"\"\"Computes SHAP values for interpretability, stores summary and dependence plots.\"\"\"\n",
    "        try:\n",
    "            if len(self.X_test) > sample_size:\n",
    "                X_sample = shap.utils.sample(self.X_test, sample_size)\n",
    "            else:\n",
    "                X_sample = self.X_test\n",
    "            explainer = shap.Explainer(model)\n",
    "            shap_values = explainer(X_sample)\n",
    "            plt.figure(figsize=(10,7))\n",
    "            shap.summary_plot(shap_values, X_sample, feature_names=self.feature_names, show=False)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{self.output_dir}/shap/summary.png\")\n",
    "            plt.close()\n",
    "            for i in range(min(5, len(self.feature_names))):\n",
    "                plt.figure(figsize=(8,5))\n",
    "                shap.dependence_plot(i, shap_values.values, X_sample, feature_names=self.feature_names, show=False)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{self.output_dir}/shap/dependence_{i}.png\")\n",
    "                plt.close()\n",
    "        except Exception as err:\n",
    "            print(\"SHAP error:\", err)\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test, name):\n",
    "        \"\"\"Quick reporting and error metrics.\"\"\"\n",
    "        pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mse = mean_squared_error(y_test, pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, pred)\n",
    "        mape = np.mean(np.abs((y_test - pred) / np.maximum(1e-8, y_test))) * 100\n",
    "        metrics = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n",
    "        print(f\"{name} results:\")\n",
    "        for k,v in metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        self._plot_predictions(y_test, pred, name)\n",
    "        return metrics\n",
    "\n",
    "    def _plot_predictions(self, y_true, y_pred, name):\n",
    "        plt.figure(figsize=(8,5))\n",
    "        idx = np.random.choice(len(y_true), min(200, len(y_true)), replace=False)\n",
    "        plt.scatter(y_true.iloc[idx], y_pred[idx], alpha=0.5)\n",
    "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "        plt.title(f'{name}: Actual vs Predicted')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/plots/{name.lower()}_prediction.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_errors(self, model):\n",
    "        pred = model.predict(self.X_test)\n",
    "        err = self.y_test - pred\n",
    "        abs_err = np.abs(err)\n",
    "        error_df = pd.DataFrame({'actual': self.y_test, 'predicted': pred, 'error': err,\n",
    "                                'abs_error': abs_err, 'datetime': self.y_test.index})\n",
    "        error_df['hour'] = error_df['datetime'].dt.hour\n",
    "        error_df['day_of_week'] = error_df['datetime'].dt.dayofweek\n",
    "        error_df['is_weekend'] = error_df['day_of_week'].isin([5, 6])\n",
    "        self._plot_error_patterns(error_df)\n",
    "        error_df.to_csv(f\"{self.output_dir}/error_analysis.csv\", index=False)\n",
    "        return error_df\n",
    "\n",
    "    def _plot_error_patterns(self, error_df):\n",
    "        plt.figure(figsize=(10,5))\n",
    "        error_df.groupby('hour')['abs_error'].mean().plot()\n",
    "        plt.title('Mean Abs Error by Hour')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/plots/error_hour.png\")\n",
    "        plt.close()\n",
    "        plt.figure(figsize=(10,5))\n",
    "        error_df.groupby('day_of_week')['abs_error'].mean().plot()\n",
    "        plt.title('Mean Abs Error by Day of Week')\n",
    "        plt.xlabel('Day of Week (0=Mon)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/plots/error_weekday.png\")\n",
    "        plt.close()\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.scatter(error_df['actual'], error_df['error'], alpha=0.3)\n",
    "        plt.axhline(0, color='r', linestyle='--')\n",
    "        plt.title('Error vs Actual Volume')\n",
    "        plt.xlabel('Actual Traffic')\n",
    "        plt.ylabel('Prediction Error')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.output_dir}/plots/error_vs_actual.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000684b2",
   "metadata": {},
   "source": [
    "## Main Pipeline Execution\n",
    "This cell sets up the traffic predictor, runs preprocessing, model training, evaluation, and error analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641553ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Model pipeline steps\n",
    "        predictor = TrafficPredictor('integrated_traffic_data.csv')\n",
    "        df = predictor.load_and_preprocess_data()\n",
    "        X = predictor.scaler.fit_transform(df[predictor.feature_names])\n",
    "        y = df['Vehicles']\n",
    "        # Train test split\n",
    "        split_idx = int(0.8 * len(X))\n",
    "        predictor.X_train, predictor.X_test = X[:split_idx], X[split_idx:]\n",
    "        predictor.y_train, predictor.y_test = y[:split_idx], y[split_idx:]\n",
    "        # Fit model and print results\n",
    "        model, metrics = predictor.train_xgboost(use_optimization=True)\n",
    "        error_df = predictor.analyze_errors(model)\n",
    "        print(\"Training and analysis complete. Results in:\", predictor.output_dir)\n",
    "    except Exception as exc:\n",
    "        print(\"Runtime error:\", exc)\n",
    "        import traceback; traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1815f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-10 09:47:09,745] A new study created in memory with name: no-name-46e25c03-29f5-4e1b-a443-074c14a6a178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Feature count: 30\n",
      "Training XGBoost...\n",
      "\n",
      "Running Optuna for hyperparam tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-10 09:47:10,137] Trial 6 finished with value: 7.139128151107272 and parameters: {'n_estimators': 57, 'max_depth': 3, 'learning_rate': 0.05398514441747595, 'subsample': 0.747250380935055, 'colsample_bytree': 0.5143912232857653}. Best is trial 6 with value: 7.139128151107272.\n",
      "[I 2025-09-10 09:47:10,329] Trial 3 finished with value: 7.788563590840444 and parameters: {'n_estimators': 96, 'max_depth': 3, 'learning_rate': 0.02275240036050781, 'subsample': 0.9356490135881741, 'colsample_bytree': 0.7646339168322039}. Best is trial 6 with value: 7.139128151107272.\n",
      "[I 2025-09-10 09:47:10,332] Trial 7 finished with value: 12.01792963847228 and parameters: {'n_estimators': 82, 'max_depth': 3, 'learning_rate': 0.010426681345545504, 'subsample': 0.5471409491355568, 'colsample_bytree': 0.9714365752253389}. Best is trial 6 with value: 7.139128151107272.\n",
      "[I 2025-09-10 09:47:10,633] Trial 4 finished with value: 3.8611682057937187 and parameters: {'n_estimators': 149, 'max_depth': 3, 'learning_rate': 0.23607149522753962, 'subsample': 0.6484929700131534, 'colsample_bytree': 0.961381140403829}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:10,934] Trial 0 finished with value: 5.95996053109192 and parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.024066936272951388, 'subsample': 0.545870062212026, 'colsample_bytree': 0.5383134251988129}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:11,253] Trial 5 finished with value: 6.590989688784646 and parameters: {'n_estimators': 296, 'max_depth': 3, 'learning_rate': 0.012250450250890796, 'subsample': 0.6468368190214278, 'colsample_bytree': 0.5335041008583823}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:11,620] Trial 2 finished with value: 5.864653799591498 and parameters: {'n_estimators': 126, 'max_depth': 7, 'learning_rate': 0.07851719013712592, 'subsample': 0.7636582037920734, 'colsample_bytree': 0.5272662747989574}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:11,792] Trial 11 finished with value: 4.6509732610635846 and parameters: {'n_estimators': 171, 'max_depth': 4, 'learning_rate': 0.07023497406569854, 'subsample': 0.5734144070590521, 'colsample_bytree': 0.5417471850749549}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:12,223] Trial 13 finished with value: 4.747910514353051 and parameters: {'n_estimators': 237, 'max_depth': 3, 'learning_rate': 0.05289155271493248, 'subsample': 0.966246328356773, 'colsample_bytree': 0.6844225837314718}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:12,267] Trial 1 finished with value: 4.906384075671611 and parameters: {'n_estimators': 291, 'max_depth': 5, 'learning_rate': 0.01737940200985073, 'subsample': 0.8798437755725685, 'colsample_bytree': 0.8622442677359624}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:12,350] Trial 14 finished with value: 5.35930983074406 and parameters: {'n_estimators': 115, 'max_depth': 4, 'learning_rate': 0.04825432950767804, 'subsample': 0.9320949021890967, 'colsample_bytree': 0.5876147933260294}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:13,796] Trial 8 finished with value: 4.438383545944706 and parameters: {'n_estimators': 262, 'max_depth': 7, 'learning_rate': 0.039595998700266984, 'subsample': 0.5595467264627818, 'colsample_bytree': 0.8723002842022147}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:13,907] Trial 9 finished with value: 5.749814090718182 and parameters: {'n_estimators': 78, 'max_depth': 10, 'learning_rate': 0.1881780842896994, 'subsample': 0.7226117287855559, 'colsample_bytree': 0.91916126815353}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:15,038] Trial 12 finished with value: 5.292825899436763 and parameters: {'n_estimators': 231, 'max_depth': 8, 'learning_rate': 0.02904444646560256, 'subsample': 0.9287875116496163, 'colsample_bytree': 0.6487374388571415}. Best is trial 4 with value: 3.8611682057937187.\n",
      "[I 2025-09-10 09:47:17,923] Trial 10 finished with value: 5.058123506187013 and parameters: {'n_estimators': 193, 'max_depth': 10, 'learning_rate': 0.060808418892360876, 'subsample': 0.7486118474208794, 'colsample_bytree': 0.9150650175865107}. Best is trial 4 with value: 3.8611682057937187.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_estimators': 149, 'max_depth': 3, 'learning_rate': 0.23607149522753962, 'subsample': 0.6484929700131534, 'colsample_bytree': 0.961381140403829}\n",
      "XGBRegressor results:\n",
      "MAE: 2.8922\n",
      "MSE: 19.9633\n",
      "RMSE: 4.4680\n",
      "R2: 0.9730\n",
      "MAPE: 18.0838\n",
      "Training and analysis complete. Results in: results\n"
     ]
    }
   ],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b943c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55aad96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6e29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
